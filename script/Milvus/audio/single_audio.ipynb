{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、创建表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import  MilvusClient, DataType, MilvusException\n",
    "import time\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://192.168.191.160:19530\"\n",
    ")\n",
    "\n",
    "# 3. Create a collection in customized setup mode\n",
    "# 3.1. Create schema\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "# 3.2. Add fields to schema\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)  # 主键字段\n",
    "schema.add_field(field_name=\"image_embedding\", datatype=DataType.FLOAT_VECTOR, dim=192)  # 向量字段\n",
    "schema.add_field(field_name=\"col_1\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_2\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_3\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_4\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_5\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_6\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_7\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_8\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_9\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_10\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_11\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_12\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_13\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_14\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_15\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_16\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_17\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_18\", datatype=DataType.INT64)  # 整数字段\n",
    "schema.add_field(field_name=\"col_19\", datatype=DataType.INT64)  # 整数字段\n",
    "\n",
    "# 3.3 Prepare index parameters\n",
    "index_params = client.prepare_index_params()\n",
    "# 3.4 Add indexs\n",
    "index_params.add_index(\n",
    "    field_name=\"id\",\n",
    "    index_type=\"STL_SORT\"\n",
    ")\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"image_embedding\", \n",
    "    index_type=\"IVF_FLAT\",\n",
    "    metric_type=\"L2\",\n",
    "    params={ \n",
    "        \"nlist\": 70\n",
    "    }\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"audio_label\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "res = client.get_load_state(\n",
    "    collection_name=\"audio_label\"\n",
    ")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、插入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "\n",
    "# 1. 设置 Milvus 客户端\n",
    "client = MilvusClient(\n",
    "    uri=\"http://192.168.191.160:19530\"\n",
    ")\n",
    "\n",
    "# 2. 定义集合名称和批量插入的批量大小\n",
    "collection_name = \"audio_label\"\n",
    "batch_size = 1000\n",
    "\n",
    "# 3. 解析文件并分批插入数据\n",
    "def load_and_insert_data(vector_file_path, label_file_path):\n",
    "    # 打开向量文件和标签文件\n",
    "    with open(vector_file_path, \"rb\") as vector_file, open(label_file_path, \"r\") as label_file:\n",
    "        batch_data = []\n",
    "        id_counter = 1  # id 从 1 开始自增\n",
    "\n",
    "        # 跳过标签文件的第一行（如果有的话，根据你的文件实际情况调整）\n",
    "        # next(label_file)  # 如果没有第一行需要跳过，可以注释掉这行\n",
    "\n",
    "        while True:\n",
    "            # 读取向量数据\n",
    "            dim = vector_file.read(4)\n",
    "            if not dim:\n",
    "                break  # 文件结束\n",
    "            dim = struct.unpack('i', dim)[0]\n",
    "            vector = struct.unpack('f' * dim, vector_file.read(4 * dim))\n",
    "\n",
    "            # 读取标签数据\n",
    "            label_line = next(label_file, None)\n",
    "            if label_line is None:\n",
    "                break  # 文件结束\n",
    "            label_line = label_line.strip()\n",
    "\n",
    "            # 解析标签行，获取 19 个数据\n",
    "            label_values = [int(x) for x in label_line.split()]\n",
    "            if len(label_values) != 19:\n",
    "                print(f\"警告: 第 {id_counter} 行有 {len(label_values)} 个值，预期为 19。\")\n",
    "                continue  # 如果数据不完整，跳过这行\n",
    "\n",
    "            # 将解析后的数据追加到批量数据列表\n",
    "            data_entry = {\n",
    "                \"id\": id_counter,  # 使用自增的 id\n",
    "                \"image_embedding\": list(vector),  # 向量数据\n",
    "            }\n",
    "            # 将 19 个值分别存入 col_1 到 col_19\n",
    "            for i in range(19):\n",
    "                data_entry[f\"col_{i+1}\"] = label_values[i]\n",
    "\n",
    "            batch_data.append(data_entry)\n",
    "\n",
    "            # id 自增\n",
    "            id_counter += 1\n",
    "\n",
    "            # 每批次插入一次数据\n",
    "            if len(batch_data) >= batch_size:\n",
    "                client.insert(collection_name=collection_name, data=batch_data)\n",
    "                print(f\"插入了 {len(batch_data)} 行数据。\")\n",
    "                batch_data = []\n",
    "\n",
    "        # 插入剩余数据\n",
    "        if batch_data:\n",
    "            client.insert(collection_name=collection_name, data=batch_data)\n",
    "            print(f\"插入了 {len(batch_data)} 行数据。\")\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "# 4. 执行数据加载和插入\n",
    "vector_file_path = os.path.join(ROOT_DIR, \"labelfilterData/datasets/audio/audio_base.fvecs\")\n",
    "label_file_path = os.path.join(ROOT_DIR, \"labelfilterData/labels/audio/labels_with_selectivity.txt\")\n",
    "load_and_insert_data(vector_file_path, label_file_path)\n",
    "\n",
    "print(\"数据导入完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、单线程脚本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil  # 用于监控进程内存\n",
    "import os\n",
    "\n",
    "# 获取当前进程\n",
    "process = psutil.Process(os.getpid())\n",
    "peak_memory_mb = 0  # 用于记录峰值内存（单位 MB）\n",
    "\n",
    "# 定义一个函数来更新峰值内存\n",
    "def update_peak_memory():\n",
    "    global peak_memory_mb\n",
    "    memory_info = process.memory_info()\n",
    "    current_memory_mb = memory_info.rss / 1024 / 1024  # 将字节转换为 MB\n",
    "    peak_memory_mb = max(peak_memory_mb, current_memory_mb)\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://222.20.98.71:19530\"\n",
    ")\n",
    "\n",
    "# 2. Function to read fvecs file\n",
    "def read_fvecs(file_path, num_vectors):\n",
    "    data = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for _ in range(num_vectors):\n",
    "            dim = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "            vector = np.frombuffer(f.read(dim * 4), dtype=np.float32)\n",
    "            data.append(vector.tolist())\n",
    "    update_peak_memory()  # 检查内存\n",
    "    return data\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"读取 .txt 文件并返回查询条件列表\"\"\"\n",
    "    conditions = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = lines[1:]  # 跳过第一行\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # 跳过空行\n",
    "                # 用空格分割，保留所有值作为条件\n",
    "                parts = line.split()\n",
    "                conditions.append(parts)  # 存储所有条件的值\n",
    "    return conditions\n",
    "\n",
    "list_1 = [\"1\", \"3_1\", \"3_2\", \"3_3\", \"3_4\", \"4\", \"5_1\", \"5_2\", \"5_3\", \"5_4\", \"6\", \"7_1\", \"7_2\", \"7_3\", \"7_4\"]\n",
    "list_2 = [[\"col_1\"], [\"col_16\"], [\"col_17\"], [\"col_18\"], [\"col_19\"], [\"col_8\"], [\"col_2\"], [\"col_3\"], [\"col_5\"], [\"col_1\"],[\"col_1\", \"col_9\", \"col_10\"],[\"col_1\", \"col_9\", \"col_16\"],[\"col_1\", \"col_9\", \"col_17\"],[\"col_1\", \"col_9\", \"col_18\"],[\"col_1\", \"col_9\", \"col_19\"]]\n",
    "list_3 = [2,3,4,5,8,10,12,20]\n",
    "\n",
    "num_vectors = 200\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "    # 文件路径\n",
    "fvecs_file = os.path.join(ROOT_DIR, \"labelfilterData/datasets/audio/audio_query.fvecs\")\n",
    "data = read_fvecs(fvecs_file, num_vectors)\n",
    "# File path and number of vectors to load\n",
    "for i, i_value in enumerate(list_1):\n",
    "    for j_value in list_3:\n",
    "        txt_file = os.path.join(ROOT_DIR, f\"labelfilterData/query_label/audio/{i_value}.txt\")\n",
    "        output_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "        conditions = read_txt_file(txt_file)\n",
    "        print(f\"Loaded {len(data)} vectors and {len(conditions)} filter conditions.\")\n",
    "\n",
    "        # Ensure the number of filters matches the number of query vectors\n",
    "        if len(conditions) != len(data):\n",
    "            raise ValueError(\"The number of filters must match the number of query vectors.\")\n",
    "\n",
    "\n",
    "\n",
    "        # Load the collection\n",
    "        client.load_collection(collection_name=\"audio_label\")\n",
    "\n",
    "        # List to store all results\n",
    "        all_results = []\n",
    "        columns = list_2[i]\n",
    "        # Start timing\n",
    "        start_time_1 = time.perf_counter()\n",
    "\n",
    "        # Perform search for each vector\n",
    "        for k in range(len(conditions)):\n",
    "            condition_values = conditions[k]\n",
    "            if len(condition_values) == 1:\n",
    "                conditions_sql = f\"{columns[0]} == {condition_values[0]}\"\n",
    "            else:\n",
    "                conditions_sql = \" && \".join([f\"{col} == {condition_values[i]}\" for i, col in enumerate(columns[:len(condition_values)])])\n",
    "            start_time = time.perf_counter()\n",
    "            res = client.search(\n",
    "                collection_name=\"audio_label\",\n",
    "                data=[data[k]],\n",
    "                filter=conditions_sql,\n",
    "                anns_field=\"image_embedding\",\n",
    "                limit=10,\n",
    "                search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": j_value}},\n",
    "                output_fields=[\"id\"]\n",
    "            )\n",
    "            end_time = time.perf_counter()\n",
    "            print(f\"Search completed in {end_time - start_time} seconds.\")\n",
    "            # Extract ids from the result structure\n",
    "            result_ids = []\n",
    "            for item in res[0]:\n",
    "                result_ids.append(str(item[\"id\"] - 1))\n",
    "\n",
    "            # Join the IDs with space and add to all_results\n",
    "            all_results.append(\" \".join(result_ids))\n",
    "            update_peak_memory()  # 在每次搜索后检查内存\n",
    "\n",
    "        # End timing   \n",
    "        end_time_1 = time.perf_counter()\n",
    "        # Write all results to the output file\n",
    "        with open(output_file, 'w') as f_out:\n",
    "            # 写入搜索结果\n",
    "            for result in all_results:\n",
    "                f_out.write(result + \"\\n\")\n",
    "            # 追加 QPS 和 Peak RES memory\n",
    "            f_out.write(f\"QPS: {200 / (end_time_1 - start_time_1):.2f}\")\n",
    "            f_out.write(f\"Peak RES memory usage: {peak_memory_mb:.2f} MB\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、单线程脚本计算召回率和QPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_ivecs(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = []\n",
    "        while True:\n",
    "            try:\n",
    "                # Read the dimension\n",
    "                width = np.fromfile(f, 'int32', 1)[0]\n",
    "                \n",
    "                # Read the vector data\n",
    "                vector = np.fromfile(f, 'int32', width)\n",
    "                \n",
    "                # Keep all elements as they are\n",
    "                data.append(vector)\n",
    "            except IndexError:\n",
    "                break  # End of file\n",
    "\n",
    "    return np.array(data, dtype=object)  # 使用dtype=object因为每行长度可能不同\n",
    "\n",
    "def get_effective_size(vector):\n",
    "    \"\"\"计算向量中有效数字的个数（遇到-1后视为无效）\"\"\"\n",
    "    for i, val in enumerate(vector):\n",
    "        if val == -1:\n",
    "            return i  # 返回第一个-1出现的位置作为有效长度\n",
    "    return len(vector)  # 如果没有-1，返回完整长度\n",
    "\n",
    "def read_output_file(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 1:\n",
    "            lines = lines[:-1]\n",
    "        return [list(map(int, line.split())) for line in lines]\n",
    "\n",
    "def extract_qps(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "            if \"QPS:\" in last_line:\n",
    "                qps_part = last_line.split(\"QPS:\")[1].split(\"Peak\")[0].strip()\n",
    "                return qps_part\n",
    "    return \"N/A\"\n",
    "\n",
    "def extract_res(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "            if \"Peak RES memory usage:\" in last_line:\n",
    "                res_part = last_line.split(\"Peak RES memory usage:\")[1].split(\"MB\")[0].strip()\n",
    "                return res_part\n",
    "    return \"N/A\"\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "recall_file = os.path.join(os.getcwd(), \"result\", f\"single_qps.out\")\n",
    "\n",
    "# 打开文件用于写入结果\n",
    "with open(recall_file, \"a\") as output_file:\n",
    "    list_1 = [\"1\", \"3_1\", \"3_2\", \"3_3\", \"3_4\", \"4\", \"5_1\", \"5_2\", \"5_3\", \"5_4\", \"6\", \"7_1\", \"7_2\", \"7_3\", \"7_4\"]\n",
    "    list_2 = [2,3,4,5,8,10,12,20]\n",
    "    \n",
    "    for i_value in list_1:\n",
    "        for j_value in list_2:\n",
    "            # Read ground truth\n",
    "            gt_file = os.path.join(ROOT_DIR, f\"labelfilterData/gt/audio/gt-query_set_{i_value}.ivecs\")\n",
    "            gt_data = read_ivecs(gt_file)\n",
    "\n",
    "            # Read result file\n",
    "            result_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "            result_data = read_output_file(result_file)\n",
    "\n",
    "            # Extract QPS value\n",
    "            qps_value = extract_qps(result_file)\n",
    "            res = extract_res(result_file)\n",
    "\n",
    "            # Calculate recall\n",
    "            total_queries = len(gt_data)\n",
    "            correct_matches = 0\n",
    "            total_possible_matches = 0\n",
    "\n",
    "            # For each query in ground truth\n",
    "            for i, gt_row in enumerate(gt_data):\n",
    "                gt_effective_size = get_effective_size(gt_row)  # 获取有效长度\n",
    "                print(gt_effective_size)\n",
    "                # 计算当前查询的正确匹配数，只考虑有效部分\n",
    "                correct_matches += sum(1 for gt_val in gt_row[:gt_effective_size] \n",
    "                                     if gt_val in result_data[i][:gt_effective_size])\n",
    "                total_possible_matches += gt_effective_size  # 累计所有可能的匹配总数\n",
    "\n",
    "            # Recall calculation using effective number of elements\n",
    "            recall = correct_matches / total_possible_matches if total_possible_matches > 0 else 0\n",
    "\n",
    "            # 写入结果到文件\n",
    "            output_file.write(f\"Recall Rate {i_value:<6} and {j_value:<4}: {recall:>6.4f}, QPS: {float(qps_value):>8.2f}, RES: {float(res):>8.2f} MB\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
