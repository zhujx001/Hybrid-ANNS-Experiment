{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、创建表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': <LoadState: Loaded>}\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import  MilvusClient, DataType, MilvusException\n",
    "import time\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://222.20.98.71:19530\"\n",
    ")\n",
    "\n",
    "# 3. Create a collection in customized setup mode\n",
    "# 3.1. Create schema\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "# 3.2. Add fields to schema\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)  # 主键字段\n",
    "schema.add_field(field_name=\"image_embedding\", datatype=DataType.FLOAT_VECTOR, dim=96)  # 向量字段\n",
    "schema.add_field(field_name=\"col_1\", datatype=DataType.INT64)  # 整数字段\n",
    "\n",
    "# 3.3 Prepare index parameters\n",
    "index_params = client.prepare_index_params()\n",
    "# 3.4 Add indexs\n",
    "index_params.add_index(\n",
    "    field_name=\"id\",\n",
    "    index_type=\"STL_SORT\"\n",
    ")\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"image_embedding\", \n",
    "    index_type=\"IVF_FLAT\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\n",
    "        \"nlist\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"deep_range\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "res = client.get_load_state(\n",
    "    collection_name=\"deep_range\"\n",
    ")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、插入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "\n",
    "# 1. 设置 Milvus 客户端\n",
    "client = MilvusClient(\n",
    "    uri=\"http://222.20.98.71:19530\"\n",
    ")\n",
    "\n",
    "# 2. 定义集合名称和批量插入的批量大小\n",
    "collection_name = \"deep_range\"\n",
    "batch_size = 1000\n",
    "\n",
    "def load_and_insert_data(vector_file_path):\n",
    "    \"\"\"\n",
    "    Load vector data from a file and insert it into the Milvus collection.\n",
    "\n",
    "    Args:\n",
    "        vector_file_path: Path to the vector file.\n",
    "    \"\"\"\n",
    "    # 打开向量文件\n",
    "    with open(vector_file_path, \"rb\") as vector_file:\n",
    "        batch_data = []\n",
    "        id_counter = 0  # id 从 1 开始自增\n",
    "\n",
    "        while True:\n",
    "            # 读取向量数据\n",
    "            dim = vector_file.read(4)\n",
    "            if not dim:\n",
    "                break  # 文件结束\n",
    "            dim = struct.unpack('i', dim)[0]\n",
    "            vector = struct.unpack('f' * dim, vector_file.read(4 * dim))\n",
    "\n",
    "            # 构造数据条目\n",
    "            data_entry = {\n",
    "                \"id\": id_counter,  # 使用自增的 id\n",
    "                \"image_embedding\": list(vector),  # 向量数据\n",
    "                \"col_1\": id_counter,  # col_1 的值与 id 相同\n",
    "            }\n",
    "\n",
    "            batch_data.append(data_entry)\n",
    "\n",
    "            # id 自增\n",
    "            id_counter += 1\n",
    "\n",
    "            # 每批次插入一次数据\n",
    "            if len(batch_data) >= batch_size:\n",
    "                client.insert(collection_name=collection_name, data=batch_data)\n",
    "                print(f\"插入了 {len(batch_data)} 行数据。\")\n",
    "                batch_data = []\n",
    "\n",
    "        # 插入剩余数据\n",
    "        if batch_data:\n",
    "            client.insert(collection_name=collection_name, data=batch_data)\n",
    "            print(f\"插入了 {len(batch_data)} 行数据。\")\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "# 4. 执行数据加载和插入\n",
    "vector_file_path = os.path.join(ROOT_DIR, \"rangefilterData/datasets/deep/deep_base.fvecs\")\n",
    "load_and_insert_data(vector_file_path)\n",
    "\n",
    "print(\"数据导入完成。\")\n",
    "\n",
    "# 1分30多秒\n",
    "# 783MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、单线程脚本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil  # 用于监控进程内存\n",
    "import os\n",
    "\n",
    "# 获取当前进程\n",
    "process = psutil.Process(os.getpid())\n",
    "peak_memory_mb = 0  # 用于记录峰值内存（单位 MB）\n",
    "\n",
    "# 定义一个函数来更新峰值内存\n",
    "def update_peak_memory():\n",
    "    global peak_memory_mb\n",
    "    memory_info = process.memory_info()\n",
    "    current_memory_mb = memory_info.rss / 1024 / 1024  # 将字节转换为 MB\n",
    "    peak_memory_mb = max(peak_memory_mb, current_memory_mb)\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://222.20.98.71:19530\"\n",
    ")\n",
    "\n",
    "# 2. Function to read fvecs file\n",
    "def read_fvecs(file_path, num_vectors):\n",
    "    data = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for _ in range(num_vectors):\n",
    "            dim = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "            vector = np.frombuffer(f.read(dim * 4), dtype=np.float32)\n",
    "            data.append(vector.tolist())\n",
    "    update_peak_memory()  # 检查内存\n",
    "    return data\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"读取 .txt 文件并返回查询条件列表\"\"\"\n",
    "    conditions = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # 跳过空行\n",
    "                # 用空格分割，获取两个值作为范围\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:\n",
    "                    conditions.append((int(parts[0]), int(parts[1])))  # 存储范围值\n",
    "    return conditions\n",
    "\n",
    "list_1 = [\"2\", \"8\"]\n",
    "list_3 = [5, 8, 10, 15, 20, 30, 50, 100]\n",
    "num_vectors = 10000\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "# 文件路径\n",
    "fvecs_file = os.path.join(ROOT_DIR, \"rangefilterData/datasets/deep/deep_query.fvecs\")\n",
    "data = read_fvecs(fvecs_file, num_vectors)\n",
    "\n",
    "# File path and number of vectors to load\n",
    "for i, i_value in enumerate(list_1):\n",
    "    for j_value in list_3:\n",
    "        txt_file = os.path.join(ROOT_DIR, f\"rangefilterData/query_range/deep/deep-96-euclidean_queries_2pow-{i_value}_ranges.txt\")\n",
    "        conditions = read_txt_file(txt_file)\n",
    "        print(f\"Loaded {len(data)} vectors and {len(conditions)} filter conditions.\")\n",
    "\n",
    "        # Ensure the number of filters matches the number of query vectors\n",
    "        if len(conditions) != len(data):\n",
    "            raise ValueError(\"The number of filters must match the number of query vectors.\")\n",
    "\n",
    "        output_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "\n",
    "        # Load the collection\n",
    "        client.load_collection(collection_name=\"deep_range\")\n",
    "\n",
    "        # List to store all results\n",
    "        all_results = []\n",
    "\n",
    "        # Start timing\n",
    "        start_time_1 = time.perf_counter()\n",
    "\n",
    "        # Perform search for each vector\n",
    "        for k in range(len(conditions)):\n",
    "            lower_bound, upper_bound = conditions[k]\n",
    "            conditions_sql = f\"col_1 >= {lower_bound} && col_1 <= {upper_bound}\"\n",
    "            start_time = time.perf_counter()\n",
    "            res = client.search(\n",
    "                collection_name=\"deep_range\",\n",
    "                data=[data[k]],\n",
    "                filter=conditions_sql,\n",
    "                anns_field=\"image_embedding\",\n",
    "                limit=10,\n",
    "                search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": j_value}},\n",
    "                output_fields=[\"id\"]\n",
    "            )\n",
    "            end_time = time.perf_counter()\n",
    "            print(f\"Search completed in {end_time - start_time} seconds.\")\n",
    "            # Extract ids from the result structure\n",
    "            result_ids = []\n",
    "            for item in res[0]:\n",
    "                result_ids.append(str(item[\"id\"] - 1))\n",
    "\n",
    "            # Join the IDs with space and add to all_results\n",
    "            all_results.append(\" \".join(result_ids))\n",
    "            update_peak_memory()  # 在每次搜索后检查内存\n",
    "\n",
    "        # End timing   \n",
    "        end_time_1 = time.perf_counter()\n",
    "        # Write all results to the output file\n",
    "        with open(output_file, 'w') as f_out:\n",
    "            # 写入搜索结果\n",
    "            for result in all_results:\n",
    "                f_out.write(result + \"\\n\")\n",
    "            # 追加 QPS 和 Peak RES memory\n",
    "            f_out.write(f\"QPS: {10000 / (end_time_1 - start_time_1):.2f}\\n\")\n",
    "            f_out.write(f\"Peak RES memory usage: {peak_memory_mb:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、单线程脚本计算召回率和QPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_ivecs(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = []\n",
    "        while True:\n",
    "            try:\n",
    "                # Read the dimension\n",
    "                width = np.fromfile(f, 'int32', 1)[0]\n",
    "                \n",
    "                # Read the vector data\n",
    "                vector = np.fromfile(f, 'int32', width)\n",
    "                \n",
    "                # Keep all elements as they are\n",
    "                data.append(vector)\n",
    "            except IndexError:\n",
    "                break  # End of file\n",
    "\n",
    "    return np.array(data, dtype=object)  # 使用dtype=object因为每行长度可能不同\n",
    "def read_txt(fname):\n",
    "    \"\"\"读取以空格分隔的文本文件，每行包含10个元素\"\"\"\n",
    "    with open(fname, \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            # 将每行的数字解析为整数列表\n",
    "            vector = list(map(int, line.strip().split()))\n",
    "            data.append(vector[:10])  # 每行只取前10个元素\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def get_effective_size(vector):\n",
    "    \"\"\"计算向量中有效数字的个数（遇到-1后视为无效）\"\"\"\n",
    "    for i, val in enumerate(vector):\n",
    "        if val == -1:\n",
    "            return i  # 返回第一个-1出现的位置作为有效长度\n",
    "    return len(vector)  # 如果没有-1，返回完整长度\n",
    "\n",
    "def read_output_file(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 1:\n",
    "            lines = lines[:-1]\n",
    "        return [list(map(int, line.split())) for line in lines]\n",
    "\n",
    "def extract_qps(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "            if \"QPS:\" in last_line:\n",
    "                qps_part = last_line.split(\"QPS:\")[1].split(\"Peak\")[0].strip()\n",
    "                return qps_part\n",
    "    return \"N/A\"\n",
    "\n",
    "def extract_res(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "            if \"Peak RES memory usage:\" in last_line:\n",
    "                res_part = last_line.split(\"Peak RES memory usage:\")[1].split(\"MB\")[0].strip()\n",
    "                return res_part\n",
    "    return \"N/A\"\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "recall_file = os.path.join(os.getcwd(), \"result\", f\"single_qps.out\")\n",
    "\n",
    "# 打开文件用于写入结果\n",
    "with open(recall_file, \"a\") as output_file:\n",
    "    list_1 = [2, 8]\n",
    "    list_2 = [5, 8, 10, 15, 20, 30, 50, 100]\n",
    "    \n",
    "    for i_value in list_1:\n",
    "        for j_value in list_2:\n",
    "            # Read ground truth\n",
    "            gt_file = os.path.join(ROOT_DIR, f\"rangefilterData/gt/deep/gt-query_set_{i_value}.ivecs\")\n",
    "            gt_data = read_ivecs(gt_file)\n",
    "            # print(gt_data)\n",
    "            result_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "            result_data = read_output_file(result_file)\n",
    "\n",
    "            # Extract QPS value\n",
    "            qps_value = extract_qps(result_file)\n",
    "            res = extract_res(result_file)\n",
    "\n",
    "            # Calculate recall\n",
    "            total_queries = len(gt_data)\n",
    "            correct_matches = 0\n",
    "            total_possible_matches = 0\n",
    "\n",
    "            # For each query in ground truth\n",
    "            for i, gt_row in enumerate(gt_data):\n",
    "                gt_effective_size = get_effective_size(gt_row)  # 获取有效长度\n",
    "                print(gt_effective_size)\n",
    "                # 计算当前查询的正确匹配数，只考虑有效部分\n",
    "                correct_matches += sum(1 for gt_val in gt_row[:gt_effective_size] \n",
    "                                     if gt_val - 1 in result_data[i][:gt_effective_size])\n",
    "                total_possible_matches += gt_effective_size  # 累计所有可能的匹配总数\n",
    "\n",
    "            # Recall calculation using effective number of elements\n",
    "            recall = correct_matches / total_possible_matches if total_possible_matches > 0 else 0\n",
    "\n",
    "            # 写入结果到文件\n",
    "            output_file.write(f\"Recall Rate {i_value:<6} and {j_value:<4}: {recall:>6.4f}, QPS: {float(qps_value):>8.2f}, RES: {float(res):>8.2f} MB\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
