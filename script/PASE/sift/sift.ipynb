{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、插入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import struct\n",
    "import os\n",
    "# 1. 设置 PostgreSQL 连接\n",
    "def create_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            database=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            host=\"222.20.98.71\",  # 你的数据库主机\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"连接数据库失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 2. 批量插入数据\n",
    "def insert_data(conn, batch_data):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        # 假设数据库表中有足够的列来接收 col_1 到 col_19\n",
    "        insert_query = \"INSERT INTO sift_label (id, image_embedding, col_1, col_2, col_3, col_4, col_5, col_6, col_7, col_8, col_9, col_10, col_11, col_12, col_13, col_14, col_15, col_16, col_17, col_18, col_19) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        cursor.executemany(insert_query, batch_data)\n",
    "        conn.commit()\n",
    "        print(f\"成功插入 {len(batch_data)} 条数据！\")\n",
    "        cursor.close()\n",
    "    except Exception as e:\n",
    "        print(f\"插入数据时出错: {e}\")\n",
    "\n",
    "# 3. 读取文件并分批插入数据\n",
    "def load_and_insert_data(vector_file_path, label_file_path):\n",
    "    # 打开向量文件和标签文件\n",
    "    with open(vector_file_path, \"rb\") as vector_file, open(label_file_path, \"r\") as label_file:\n",
    "        batch_data = []\n",
    "        id_counter = 1  # id 从 1 开始自增\n",
    "        batch_size = 1000  # 每批次插入1000条数据\n",
    "\n",
    "        # 创建数据库连接\n",
    "        conn = create_connection()\n",
    "        if not conn:\n",
    "            print(\"无法连接到数据库，终止操作。\")\n",
    "            return\n",
    "\n",
    "        # 不再跳过第一行\n",
    "        for label_line in label_file:\n",
    "            # 读取向量数据\n",
    "            dim_bytes = vector_file.read(4)\n",
    "            if not dim_bytes:\n",
    "                break  # 文件结束\n",
    "            dim = struct.unpack('i', dim_bytes)[0]\n",
    "            vector = struct.unpack('f' * dim, vector_file.read(4 * dim))\n",
    "\n",
    "            # 解析标签行\n",
    "            label_values = label_line.strip().split()\n",
    "            if len(label_values) != 19:\n",
    "                print(f\"警告：标签行中数据数量不正确（应为19个），实际数量为 {len(label_values)}。跳过该行。\")\n",
    "                continue\n",
    "\n",
    "            # 将解析后的数据追加到批量数据列表\n",
    "            batch_data.append((id_counter, list(vector), *label_values))\n",
    "\n",
    "            # 检查是否达到批量大小，如果是，则插入数据库并清空批量数据列表\n",
    "            if len(batch_data) == batch_size:\n",
    "                insert_data(conn, batch_data)\n",
    "                batch_data = []\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "        # 插入剩余的批量数据\n",
    "        if batch_data:\n",
    "            insert_data(conn, batch_data)\n",
    "\n",
    "        conn.close()  # 关闭数据库连接\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "# 4. 执行数据加载和插入\n",
    "vector_file_path = os.path.join(ROOT_DIR, \"labelfilterData/datasets/sift/sift_base.fvecs\")\n",
    "label_file_path = os.path.join(ROOT_DIR, \"labelfilterData/labels/sift/labels_with_selectivity.txt\")\n",
    "load_and_insert_data(vector_file_path, label_file_path)\n",
    "\n",
    "print(\"数据导入完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、索引构建时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "import time\n",
    "\n",
    "# 创建数据库连接\n",
    "def create_connection():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            database=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            host=\"222.20.98.71\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        print(\"连接成功！\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"连接失败: {e}\")\n",
    "    return conn\n",
    "\n",
    "# 执行索引构建SQL\n",
    "def create_index(conn):\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        \n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 执行索引构建\n",
    "        create_index_sql = \"\"\"\n",
    "            CREATE INDEX pase_sift_16_200\n",
    "            ON sift_label\n",
    "            USING pase_hnsw(image_embedding)\n",
    "            WITH (dim = 128, base_nb_num = 16, ef_build = 200, ef_search = 200, base64_encoded = 0);\n",
    "        \"\"\"\n",
    "        cursor.execute(create_index_sql)\n",
    "        conn.commit()  # 提交事务\n",
    "\n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        index_build_time = end_time - start_time\n",
    "        \n",
    "        print(f\"索引构建完成！\")\n",
    "        print(f\"索引构建时间: {index_build_time:.2f} 秒\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"索引构建失败: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def main():\n",
    "    # 创建数据库连接\n",
    "    connection = create_connection()\n",
    "    \n",
    "    if connection:\n",
    "        try:\n",
    "            # 执行索引构建\n",
    "            create_index(connection)\n",
    "        finally:\n",
    "            connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、单线程测试脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import struct\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 设置 PostgreSQL 连接\n",
    "def create_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            database=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            host=\"222.20.98.71\",  # 你的数据库主机\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"连接数据库失败: {e}\")\n",
    "        return None\n",
    "    \n",
    "def read_fvecs_file(file_path):\n",
    "    \"\"\"读取 .fvecs 文件并返回所有向量\"\"\"\n",
    "    query_vectors = []\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while True:\n",
    "            # 读取向量维度\n",
    "            dim_bytes = f.read(4)\n",
    "            if not dim_bytes:\n",
    "                break\n",
    "            dim = struct.unpack('i', dim_bytes)[0]\n",
    "            # 读取向量数据\n",
    "            vector_bytes = f.read(dim * 4)\n",
    "            if not vector_bytes:\n",
    "                break\n",
    "            vector = struct.unpack(f'{dim}f', vector_bytes)\n",
    "            query_vectors.append(vector)\n",
    "    return query_vectors\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"读取 .txt 文件并返回查询条件列表\"\"\"\n",
    "    conditions = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # 跳过空行\n",
    "                # 用空格分割，保留所有值作为条件\n",
    "                parts = line.split()\n",
    "                conditions.append(parts)  # 存储所有条件的值\n",
    "    return conditions\n",
    "def execute_query(conn, query, params, output_file, j_value, total_time, k):\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SET enable_seqscan = off;\")  # 设置索引扫描\n",
    "        cursor.execute(\"SET enable_indexscan = on;\")  # 设置索引扫描\n",
    "        cursor.execute(f\"SET hnsw.ef_search = {j_value};\")  # 设置索引扫描\n",
    "        # 设置索引扫描和关闭顺序扫描\n",
    "        start_time = time.time()  # 开始计时\n",
    "        # 执行查询\n",
    "        cursor.execute(query, params)\n",
    "        result = cursor.fetchall()  # 获取查询结果\n",
    "        end_time = time.time()  # 结束计时\n",
    "        print(f\"查询执行时间: {end_time - start_time:.6f} 秒\")\n",
    "        if k != 0:\n",
    "            total_time += (end_time - start_time)  # 累加查询时间\n",
    "\n",
    "        # 将查询结果写入文件\n",
    "        with open(output_file, \"a\") as f:\n",
    "            ids = [str(row[0] - 1) for row in result]  # 提取 ID 列\n",
    "            f.write(\" \".join(ids) + \"\\n\")  # 每次查询结果写在一行\n",
    "    except Exception as e:\n",
    "        print(f\"查询失败: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    return total_time\n",
    "def main():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "    # 文件路径\n",
    "    fvecs_file = os.path.join(ROOT_DIR, \"labelfilterData/datasets/sift/sift_query.fvecs\")\n",
    "\n",
    "    list_1 = [\"1\", \"3_1\", \"3_2\", \"3_3\", \"3_4\", \"4\", \"5_1\", \"5_2\", \"5_3\", \"5_4\"]\n",
    "    list_2 = [[\"col_1\"], [\"col_16\"], [\"col_17\"], [\"col_18\"], [\"col_19\"], [\"col_8\"], [\"col_2\"], [\"col_3\"], [\"col_5\"], [\"col_1\"]]\n",
    "    list_3 = [100, 150, 200, 250, 400, 1000]\n",
    "\n",
    "\n",
    "    # 读取向量数据和查询条件\n",
    "    query_vectors = read_fvecs_file(fvecs_file)\n",
    "   \n",
    "    for i, i_value in enumerate(list_1):\n",
    "        for j_value in list_3:\n",
    "            # 创建数据库连接\n",
    "            connection = create_connection()\n",
    "            if connection:\n",
    "                try:\n",
    "                    txt_file = os.path.join(ROOT_DIR, f\"labelfilterData/query_label/sift/{i_value}.txt\")\n",
    "                    output_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "                    conditions = read_txt_file(txt_file)\n",
    "                    # 清空结果文件，确保每次执行时写入的是最新的查询结果\n",
    "                    with open(output_file, \"w\") as f:\n",
    "                        f.truncate(0)\n",
    "\n",
    "                        # 定义数据库表的列名，与条件对应\n",
    "                        columns = list_2[i]\n",
    "                        total_time = 0\n",
    "                        # 遍历查询条件\n",
    "                        for k in range(len(conditions)):\n",
    "                        # for k in range(len(conditions)):\n",
    "                            condition_values = conditions[k]\n",
    "                            query_vector = query_vectors[k]\n",
    "                            # 构造 SQL 查询语句\n",
    "                            # 动态生成 WHERE 条件\n",
    "                            if len(condition_values) == 1:\n",
    "                                conditions_sql = f\"{columns[0]} = %s\"\n",
    "                            else:\n",
    "                                conditions_sql = \" AND \".join([f\"{col} = %s\" for col in columns[:len(condition_values)]])\n",
    "                            query = f\"\"\"\n",
    "                                SELECT id\n",
    "                                FROM sift_label\n",
    "                                WHERE {conditions_sql}\n",
    "                                ORDER BY image_embedding <?> '{','.join(map(str, query_vector))}'::pase ASC\n",
    "                                LIMIT 10;\n",
    "                            \"\"\"\n",
    "                            print(f\"正在执行查询条件: {condition_values}...\")\n",
    "                            total_time = execute_query(connection, query, tuple(condition_values), output_file, j_value, total_time, k)\n",
    "                        qps = len(conditions) / total_time if total_time != 0 else 0\n",
    "                        # 将 QPS 写入 output_file\n",
    "                        with open(output_file, \"a\") as f:  # 使用追加模式\n",
    "                            f.write(f\"\\nQPS: {qps}\\n\")\n",
    "                finally:\n",
    "                    connection.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、召回率计算脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_ivecs(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = []\n",
    "        while True:\n",
    "            try:\n",
    "                # Read the dimension\n",
    "                width = np.fromfile(f, 'int32', 1)[0]\n",
    "                \n",
    "                # Read the vector data\n",
    "                vector = np.fromfile(f, 'int32', width)\n",
    "                \n",
    "                # If the vector is longer than 10, we only take the first 10 elements\n",
    "                data.append(vector[:10])\n",
    "            except IndexError:\n",
    "                break  # End of file\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "def read_output_file(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()  # 读取所有行\n",
    "        if len(lines) > 2:  # 如果文件行数大于2，忽略最后两行\n",
    "            lines = lines[:-2]\n",
    "        return [list(map(int, line.split())) for line in lines]\n",
    "\n",
    "def extract_qps(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()  # 获取最后一行并去除首尾空格\n",
    "            if \"QPS:\" in last_line:\n",
    "                qps_value = last_line.split(\"QPS:\")[-1].strip()  # 提取 QPS 值\n",
    "                return qps_value\n",
    "    return \"N/A\"  # 如果没有找到 QPS 值，返回 N/A\n",
    "\n",
    "# 定义结果文件路径\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/Experiment\"))\n",
    "recall_file = os.path.join(os.getcwd(), \"result\", f\"single_qps.out\")\n",
    "\n",
    "# 打开文件用于写入结果\n",
    "with open(recall_file, \"w\") as output_file:\n",
    "    list_1 = [\"1\", \"3_1\", \"3_2\", \"3_3\", \"3_4\", \"4\", \"5_1\", \"5_2\", \"5_3\", \"5_4\"]\n",
    "    list_2 = [100, 150, 200, 250, 400, 1000]\n",
    "\n",
    "    for i_value in list_1:\n",
    "        for j_value in list_2:\n",
    "            # Read ground truth\n",
    "            gt_file = os.path.join(ROOT_DIR, f\"labelfilterData/gt/sift/gt-query_set_{i_value}.ivecs\")\n",
    "            gt_data = read_ivecs(gt_file)\n",
    "\n",
    "            # Read result file\n",
    "            result_file = os.path.join(os.getcwd(), \"result\", f\"{i_value}_16_200_{j_value}.out\")\n",
    "            result_data = read_output_file(result_file)\n",
    "\n",
    "            # Extract QPS value\n",
    "            qps_value = extract_qps(result_file)\n",
    "\n",
    "            # Calculate recall\n",
    "            total_queries = len(gt_data)\n",
    "            correct_matches = 0\n",
    "\n",
    "            # For each query in ground truth\n",
    "            for i, gt_row in enumerate(gt_data):\n",
    "                # Check if any of the first 10 elements from ground truth are in the result's top 10\n",
    "                correct_matches += sum(1 for gt_val in gt_row if gt_val in result_data[i][:10])\n",
    "\n",
    "            # Recall calculation\n",
    "            recall = correct_matches / (total_queries * 10)  # Since each query has 10 elements to match\n",
    "\n",
    "            # 写入结果到文件\n",
    "            output_file.write(f\"Recall Rate {i_value:<6} and {j_value:<4}: {recall:>6.4f}, QPS: {float(qps_value):>8.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
